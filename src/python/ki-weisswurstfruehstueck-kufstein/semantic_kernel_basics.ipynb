{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel with Azure OpenAI âš¡\n",
    "\n",
    "This notebook is a simple demonstration of how to use the [Semantic Kernel](https://learn.microsoft.com/semantic-kernel/overview/?WT.mc_id=AZ-MVP-5003203) with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service?WT.mc_id=AZ-MVP-5003203). \n",
    "\n",
    "## Install requirements\n",
    "Make sure you have configured the necessary settings for the Azure OpenAI Connector. See the [documentation](https://github.com/microsoft/semantic-kernel/blob/main/python/README.md#openai--azure-openai-api-keys).\n",
    "\n",
    "We only need to install the Semantic Kernel SDK from pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting semantic-kernel\n",
      "  Using cached semantic_kernel-1.11.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting aiohttp~=3.8 (from semantic-kernel)\n",
      "  Downloading aiohttp-3.10.9-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting pydantic~=2.0 (from semantic-kernel)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pydantic-settings~=2.0 (from semantic-kernel)\n",
      "  Using cached pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting defusedxml~=0.7 (from semantic-kernel)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting azure-identity~=1.13 (from semantic-kernel)\n",
      "  Downloading azure_identity-1.18.0-py3-none-any.whl.metadata (81 kB)\n",
      "Collecting numpy>=1.26.0 (from semantic-kernel)\n",
      "  Downloading numpy-2.1.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Collecting openai~=1.0 (from semantic-kernel)\n",
      "  Using cached openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting openapi_core<0.20,>=0.18 (from semantic-kernel)\n",
      "  Using cached openapi_core-0.19.4-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting opentelemetry-api~=1.24 (from semantic-kernel)\n",
      "  Using cached opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-sdk~=1.24 (from semantic-kernel)\n",
      "  Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting prance~=23.6.21.0 (from semantic-kernel)\n",
      "  Using cached prance-23.6.21.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pybars4~=0.9 (from semantic-kernel)\n",
      "  Using cached pybars4-0.9.13-py3-none-any.whl\n",
      "Collecting jinja2~=3.1 (from semantic-kernel)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: nest-asyncio~=1.6 in d:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel) (1.6.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp~=3.8->semantic-kernel)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp~=3.8->semantic-kernel)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp~=3.8->semantic-kernel)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp~=3.8->semantic-kernel)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp~=3.8->semantic-kernel)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp~=3.8->semantic-kernel)\n",
      "  Downloading yarl-1.13.1-cp312-cp312-win_amd64.whl.metadata (52 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity~=1.13->semantic-kernel)\n",
      "  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity~=1.13->semantic-kernel)\n",
      "  Downloading cryptography-43.0.1-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity~=1.13->semantic-kernel)\n",
      "  Downloading msal-1.31.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity~=1.13->semantic-kernel)\n",
      "  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from azure-identity~=1.13->semantic-kernel)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2~=3.1->semantic-kernel)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai~=1.0->semantic-kernel)\n",
      "  Downloading anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai~=1.0->semantic-kernel)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai~=1.0->semantic-kernel)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai~=1.0->semantic-kernel)\n",
      "  Using cached jiter-0.5.0-cp312-none-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting sniffio (from openai~=1.0->semantic-kernel)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai~=1.0->semantic-kernel)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting isodate (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.18.0 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting jsonschema-path<0.4.0,>=0.3.1 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached jsonschema_path-0.3.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting more-itertools (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached openapi_schema_validator-0.6.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting openapi-spec-validator<0.8.0,>=0.7.1 (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached openapi_spec_validator-0.7.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting parse (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting werkzeug (from openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api~=1.24->semantic-kernel)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api~=1.24->semantic-kernel)\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-sdk~=1.24->semantic-kernel)\n",
      "  Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting chardet>=3.0 (from prance~=23.6.21.0->semantic-kernel)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting ruamel.yaml>=0.17.10 (from prance~=23.6.21.0->semantic-kernel)\n",
      "  Using cached ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting requests>=2.25 (from prance~=23.6.21.0->semantic-kernel)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six~=1.15 in d:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\lib\\site-packages (from prance~=23.6.21.0->semantic-kernel) (24.1)\n",
      "Collecting PyMeta3>=0.5.1 (from pybars4~=0.9->semantic-kernel)\n",
      "  Using cached PyMeta3-0.5.1-py3-none-any.whl\n",
      "Collecting annotated-types>=0.6.0 (from pydantic~=2.0->semantic-kernel)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic~=2.0->semantic-kernel)\n",
      "  Downloading pydantic_core-2.23.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings~=2.0->semantic-kernel)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai~=1.0->semantic-kernel)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity~=1.13->semantic-kernel)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api~=1.24->semantic-kernel)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai~=1.0->semantic-kernel)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai~=1.0->semantic-kernel)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.0->semantic-kernel)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api~=1.24->semantic-kernel)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached rpds_py-0.20.0-cp312-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting PyYAML>=5.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting pathable<0.5.0,>=0.4.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached pathable-0.4.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity~=1.13->semantic-kernel)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting portalocker<3,>=1.4 (from msal-extensions>=1.2.0->azure-identity~=1.13->semantic-kernel)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting rfc3339-validator (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel)\n",
      "  Using cached lazy_object_proxy-1.10.0-cp312-cp312-win_amd64.whl.metadata (8.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.25->prance~=23.6.21.0->semantic-kernel)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.25->prance~=23.6.21.0->semantic-kernel)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.10->prance~=23.6.21.0->semantic-kernel)\n",
      "  Using cached ruamel.yaml.clib-0.2.8-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama in d:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\lib\\site-packages (from tqdm>4->openai~=1.0->semantic-kernel) (0.4.6)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity~=1.13->semantic-kernel)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity~=1.13->semantic-kernel) (307)\n",
      "Using cached semantic_kernel-1.11.0-py3-none-any.whl (472 kB)\n",
      "Downloading aiohttp-3.10.9-cp312-cp312-win_amd64.whl (379 kB)\n",
      "Downloading azure_identity-1.18.0-py3-none-any.whl (187 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading numpy-2.1.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 11.0/12.6 MB 52.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 52.4 MB/s eta 0:00:00\n",
      "Using cached openai-1.51.0-py3-none-any.whl (383 kB)\n",
      "Using cached openapi_core-0.19.4-py3-none-any.whl (103 kB)\n",
      "Using cached opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "Using cached prance-23.6.21.0-py3-none-any.whl (36 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 53.4 MB/s eta 0:00:00\n",
      "Using cached pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading cryptography-43.0.1-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.1/3.1 MB 59.8 MB/s eta 0:00:00\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Downloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached jiter-0.5.0-cp312-none-win_amd64.whl (189 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached jsonschema_path-0.3.3-py3-none-any.whl (14 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
      "Downloading msal-1.31.0-py3-none-any.whl (113 kB)\n",
      "Downloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached openapi_schema_validator-0.6.2-py3-none-any.whl (8.8 kB)\n",
      "Using cached openapi_spec_validator-0.7.1-py3-none-any.whl (38 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading yarl-1.13.1-cp312-cp312-win_amd64.whl (111 kB)\n",
      "Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Using cached parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached lazy_object_proxy-1.10.0-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Using cached pathable-0.4.3-py3-none-any.whl (9.6 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.20.0-cp312-none-win_amd64.whl (214 kB)\n",
      "Using cached ruamel.yaml.clib-0.2.8-cp312-cp312-win_amd64.whl (115 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-win_amd64.whl (37 kB)\n",
      "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: PyMeta3, parse, zipp, wrapt, urllib3, typing-extensions, tqdm, sniffio, ruamel.yaml.clib, rpds-py, rfc3339-validator, PyYAML, python-dotenv, PyJWT, pycparser, pybars4, portalocker, pathable, numpy, multidict, more-itertools, MarkupSafe, lazy-object-proxy, jiter, isodate, idna, h11, frozenlist, distro, defusedxml, charset-normalizer, chardet, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, werkzeug, ruamel.yaml, requests, referencing, pydantic-core, jinja2, importlib-metadata, httpcore, deprecated, cffi, anyio, aiosignal, pydantic, prance, opentelemetry-api, jsonschema-specifications, jsonschema-path, httpx, cryptography, azure-core, aiohttp, pydantic-settings, opentelemetry-semantic-conventions, openai, jsonschema, opentelemetry-sdk, openapi-schema-validator, msal, openapi-spec-validator, msal-extensions, openapi_core, azure-identity, semantic-kernel\n",
      "Successfully installed MarkupSafe-2.1.5 PyJWT-2.9.0 PyMeta3-0.5.1 PyYAML-6.0.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.0 attrs-24.2.0 azure-core-1.31.0 azure-identity-1.18.0 certifi-2024.8.30 cffi-1.17.1 chardet-5.2.0 charset-normalizer-3.3.2 cryptography-43.0.1 defusedxml-0.7.1 deprecated-1.2.14 distro-1.9.0 frozenlist-1.4.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 idna-3.10 importlib-metadata-8.4.0 isodate-0.6.1 jinja2-3.1.4 jiter-0.5.0 jsonschema-4.23.0 jsonschema-path-0.3.3 jsonschema-specifications-2023.12.1 lazy-object-proxy-1.10.0 more-itertools-10.5.0 msal-1.31.0 msal-extensions-1.2.0 multidict-6.1.0 numpy-2.1.2 openai-1.51.0 openapi-schema-validator-0.6.2 openapi-spec-validator-0.7.1 openapi_core-0.19.4 opentelemetry-api-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 parse-1.20.2 pathable-0.4.3 portalocker-2.10.1 prance-23.6.21.0 pybars4-0.9.13 pycparser-2.22 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.5.2 python-dotenv-1.0.1 referencing-0.35.1 requests-2.32.3 rfc3339-validator-0.1.4 rpds-py-0.20.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 semantic-kernel-1.11.0 sniffio-1.3.1 tqdm-4.66.5 typing-extensions-4.12.2 urllib3-2.2.3 werkzeug-3.0.4 wrapt-1.16.0 yarl-1.13.1 zipp-3.20.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install semantic-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "service_id=\"chat-gpt\"\n",
    "\n",
    "kernel.add_service(\n",
    "  AzureChatCompletion(\n",
    "      service_id=service_id,\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Plugins\n",
    "The following script will add all plugins from the `plugins_directory` to the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugins_directory = \"Plugins\"\n",
    "kubernetesPlugin = kernel.add_plugin(parent_directory=plugins_directory, plugin_name=\"KubernetesPlugin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the KubectlPrompt Plugin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function failed. Error: Error occurred while invoking function KubectlPrompt: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error\n",
      "Something went wrong in function invocation. During function invocation: 'KubernetesPlugin-KubectlPrompt'. Error description: 'Error occurred while invoking function KubectlPrompt: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error'\n"
     ]
    },
    {
     "ename": "KernelInvokeException",
     "evalue": "Error occurred while invoking function: 'KubernetesPlugin-KubectlPrompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:47\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, request_settings)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_structured_output(request_settings, settings)\n\u001b[1;32m---> 47\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1490\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1489\u001b[0m validate_response_format(response_format)\n\u001b[1;32m-> 1490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1492\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   1493\u001b[0m         {\n\u001b[0;32m   1494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1495\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1496\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1499\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1500\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1503\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1504\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1505\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1506\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1507\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1508\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1509\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1510\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1516\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1517\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1518\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1520\u001b[0m         },\n\u001b[0;32m   1521\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m   1522\u001b[0m     ),\n\u001b[0;32m   1523\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1524\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1525\u001b[0m     ),\n\u001b[0;32m   1526\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1527\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1528\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   1529\u001b[0m )\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1831\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1828\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1829\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1830\u001b[0m )\n\u001b[1;32m-> 1831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1525\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1523\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1526\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1527\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1528\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1529\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1530\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1626\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1625\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1626\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1629\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1630\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1635\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mContentFilterAIException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:171\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     chat_message_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m prompt_render_result\u001b[38;5;241m.\u001b[39mai_service\u001b[38;5;241m.\u001b[39mget_chat_message_contents(\n\u001b[0;32m    172\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[0;32m    173\u001b[0m         settings\u001b[38;5;241m=\u001b[39mprompt_render_result\u001b[38;5;241m.\u001b[39mexecution_settings,\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m: context\u001b[38;5;241m.\u001b[39mkernel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m: context\u001b[38;5;241m.\u001b[39marguments},\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:134\u001b[0m, in \u001b[0;36mChatCompletionClientBase.get_chat_message_contents\u001b[1;34m(self, chat_history, settings, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m     settings\u001b[38;5;241m.\u001b[39mfunction_choice_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfunction_choice_behavior\u001b[38;5;241m.\u001b[39mauto_invoke_kernel_functions\n\u001b[0;32m    133\u001b[0m ):\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_get_chat_message_contents(chat_history, settings)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Auto invoke loop\u001b[39;00m\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:83\u001b[0m, in \u001b[0;36mtrace_chat_completion.<locals>.inner_trace_chat_completion.<locals>.wrapper_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     85\u001b[0m completion_service: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatCompletionClientBase\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:88\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase._inner_get_chat_message_contents\u001b[1;34m(self, chat_history, settings)\u001b[0m\n\u001b[0;32m     86\u001b[0m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_id\n\u001b[1;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(request_settings\u001b[38;5;241m=\u001b[39msettings)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:54\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, request_settings)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_filter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service encountered a content error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m         ex,\n\u001b[0;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m     ex,\n\u001b[0;32m     61\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mContentFilterAIException\u001b[0m: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFunctionExecutionException\u001b[0m                Traceback (most recent call last)",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\kernel.py:201\u001b[0m, in \u001b[0;36mKernel.invoke\u001b[1;34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m function\u001b[38;5;241m.\u001b[39minvoke(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, arguments\u001b[38;5;241m=\u001b[39marguments, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationCancelledException \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:245\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[1;34m(self, kernel, arguments, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(current_span, e, attributes)\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:237\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[1;34m(self, kernel, arguments, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m stack \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mconstruct_call_stack(\n\u001b[0;32m    234\u001b[0m     filter_type\u001b[38;5;241m=\u001b[39mFilterTypes\u001b[38;5;241m.\u001b[39mFUNCTION_INVOCATION,\n\u001b[0;32m    235\u001b[0m     inner_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_internal,\n\u001b[0;32m    236\u001b[0m )\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m stack(function_context)\n\u001b[0;32m    239\u001b[0m KernelFunctionLogMessages\u001b[38;5;241m.\u001b[39mlog_function_invoked_success(logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfully_qualified_name)\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:177\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FunctionExecutionException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_message_contents:\n",
      "\u001b[1;31mFunctionExecutionException\u001b[0m: Error occurred while invoking function KubectlPrompt: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m kernel\u001b[38;5;241m.\u001b[39minvoke(kubernetesPlugin[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKubectlPrompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist all namespace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\kernel.py:210\u001b[0m, in \u001b[0;36mKernel.invoke\u001b[1;34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    206\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething went wrong in function invocation. During function invocation:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Error description: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m KernelInvokeException(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m: Error occurred while invoking function: 'KubernetesPlugin-KubectlPrompt'"
     ]
    }
   ],
   "source": [
    "result = await kernel.invoke(kubernetesPlugin[\"MyKube\"], input=\"list all namespace\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the KubeMatch Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function failed. Error: Error occurred while invoking function KubectlPrompt: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error\n",
      "Something went wrong in function invocation. During function invocation: 'KubernetesPlugin-KubectlPrompt'. Error description: 'Error occurred while invoking function KubectlPrompt: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error'\n"
     ]
    },
    {
     "ename": "KernelInvokeException",
     "evalue": "Error occurred while invoking function: 'KubernetesPlugin-KubectlPrompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:47\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, request_settings)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_structured_output(request_settings, settings)\n\u001b[1;32m---> 47\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1490\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1489\u001b[0m validate_response_format(response_format)\n\u001b[1;32m-> 1490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1492\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   1493\u001b[0m         {\n\u001b[0;32m   1494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1495\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1496\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1499\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1500\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1503\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1504\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1505\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1506\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1507\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1508\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1509\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1510\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1516\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1517\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1518\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1520\u001b[0m         },\n\u001b[0;32m   1521\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m   1522\u001b[0m     ),\n\u001b[0;32m   1523\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1524\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1525\u001b[0m     ),\n\u001b[0;32m   1526\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1527\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1528\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   1529\u001b[0m )\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1831\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1828\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1829\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1830\u001b[0m )\n\u001b[1;32m-> 1831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1525\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1523\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1526\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1527\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1528\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1529\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1530\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1531\u001b[0m )\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1626\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1625\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1626\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1629\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1630\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1635\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mContentFilterAIException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:171\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     chat_message_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m prompt_render_result\u001b[38;5;241m.\u001b[39mai_service\u001b[38;5;241m.\u001b[39mget_chat_message_contents(\n\u001b[0;32m    172\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39mchat_history,\n\u001b[0;32m    173\u001b[0m         settings\u001b[38;5;241m=\u001b[39mprompt_render_result\u001b[38;5;241m.\u001b[39mexecution_settings,\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m: context\u001b[38;5;241m.\u001b[39mkernel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m: context\u001b[38;5;241m.\u001b[39marguments},\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:134\u001b[0m, in \u001b[0;36mChatCompletionClientBase.get_chat_message_contents\u001b[1;34m(self, chat_history, settings, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m     settings\u001b[38;5;241m.\u001b[39mfunction_choice_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfunction_choice_behavior\u001b[38;5;241m.\u001b[39mauto_invoke_kernel_functions\n\u001b[0;32m    133\u001b[0m ):\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_get_chat_message_contents(chat_history, settings)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Auto invoke loop\u001b[39;00m\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:83\u001b[0m, in \u001b[0;36mtrace_chat_completion.<locals>.inner_trace_chat_completion.<locals>.wrapper_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     85\u001b[0m completion_service: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatCompletionClientBase\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:88\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase._inner_get_chat_message_contents\u001b[1;34m(self, chat_history, settings)\u001b[0m\n\u001b[0;32m     86\u001b[0m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_id\n\u001b[1;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(request_settings\u001b[38;5;241m=\u001b[39msettings)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:54\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, request_settings)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_filter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service encountered a content error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m         ex,\n\u001b[0;32m     57\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m     ex,\n\u001b[0;32m     61\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mContentFilterAIException\u001b[0m: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFunctionExecutionException\u001b[0m                Traceback (most recent call last)",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\kernel.py:201\u001b[0m, in \u001b[0;36mKernel.invoke\u001b[1;34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m function\u001b[38;5;241m.\u001b[39minvoke(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, arguments\u001b[38;5;241m=\u001b[39marguments, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationCancelledException \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:245\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[1;34m(self, kernel, arguments, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(current_span, e, attributes)\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:237\u001b[0m, in \u001b[0;36mKernelFunction.invoke\u001b[1;34m(self, kernel, arguments, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m stack \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mconstruct_call_stack(\n\u001b[0;32m    234\u001b[0m     filter_type\u001b[38;5;241m=\u001b[39mFilterTypes\u001b[38;5;241m.\u001b[39mFUNCTION_INVOCATION,\n\u001b[0;32m    235\u001b[0m     inner_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_internal,\n\u001b[0;32m    236\u001b[0m )\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m stack(function_context)\n\u001b[0;32m    239\u001b[0m KernelFunctionLogMessages\u001b[38;5;241m.\u001b[39mlog_function_invoked_success(logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfully_qualified_name)\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:177\u001b[0m, in \u001b[0;36mKernelFunctionFromPrompt._invoke_internal\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FunctionExecutionException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_message_contents:\n",
      "\u001b[1;31mFunctionExecutionException\u001b[0m: Error occurred while invoking function KubectlPrompt: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service encountered a content error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m goodFit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mThis project involves developing a high-traffic e-commerce platform that expects dynamic scaling and high availability to handle seasonal peaks in user traffic. \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mThe architecture is based on microservices, each responsible for different aspects such as user authentication, product catalog, and payment processing. \u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mThe team is experienced in containerized environments and requires a solution that allows for efficient deployment, scaling, and management of services, with robust orchestration capabilities.\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m kernel\u001b[38;5;241m.\u001b[39minvoke(kubernetesPlugin[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKubectlPrompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mgoodFit)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32md:\\source\\github\\whiteduck\\demo-ai-semantic-kernel\\.venv\\Lib\\site-packages\\semantic_kernel\\kernel.py:210\u001b[0m, in \u001b[0;36mKernel.invoke\u001b[1;34m(self, function, arguments, function_name, plugin_name, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    206\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething went wrong in function invocation. During function invocation:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Error description: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m KernelInvokeException(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred while invoking function: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39mfully_qualified_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mKernelInvokeException\u001b[0m: Error occurred while invoking function: 'KubernetesPlugin-KubectlPrompt'"
     ]
    }
   ],
   "source": [
    "goodFit=\"\"\"\n",
    "This project involves developing a high-traffic e-commerce platform that expects dynamic scaling and high availability to handle seasonal peaks in user traffic. \n",
    "The architecture is based on microservices, each responsible for different aspects such as user authentication, product catalog, and payment processing. \n",
    "The team is experienced in containerized environments and requires a solution that allows for efficient deployment, scaling, and management of services, with robust orchestration capabilities.\n",
    "\"\"\"\n",
    "\n",
    "result = await kernel.invoke(kubernetesPlugin[\"KubeMatch\"], input=goodFit)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided project description and the factors to consider, here is an assessment of whether Kubernetes is a suitable platform for deployment:\n",
      "\n",
      "### Scalability\n",
      "- **Assessment**: The project has minimal scalability requirements and is expected to have a stable and predictable load.\n",
      "- **Conclusion**: Kubernetes is designed for dynamic scaling, which is not necessary for this project. Therefore, Kubernetes may be overkill for the scalability needs of this project.\n",
      "\n",
      "### Manageability\n",
      "- **Assessment**: While Kubernetes offers centralized management for containers, the project is small-scale and involves a simple monolithic architecture.\n",
      "- **Conclusion**: The benefits of Kubernetes' centralized management are less significant for a small-scale project with minimal complexity.\n",
      "\n",
      "### Microservices Architecture\n",
      "- **Assessment**: The project is built with a monolithic architecture.\n",
      "- **Conclusion**: Kubernetes excels in managing microservices architectures, but this project does not follow that pattern. Therefore, Kubernetes may not be the best fit.\n",
      "\n",
      "### Deployment Complexity\n",
      "- **Assessment**: Deploying and maintaining a Kubernetes environment involves significant complexity.\n",
      "- **Conclusion**: Given the simplicity of the project, the overhead of setting up and maintaining Kubernetes is not justified.\n",
      "\n",
      "### Team Expertise\n",
      "- **Assessment**: The team lacks experience with containerization and Kubernetes.\n",
      "- **Conclusion**: The learning curve and training costs to get the team up to speed with Kubernetes would be high and are not justified for this simple project.\n",
      "\n",
      "### Security Requirements\n",
      "- **Assessment**: Kubernetes offers robust security features, but the project description does not specify any advanced security needs.\n",
      "- **Conclusion**: The security features of Kubernetes may not be necessary for this project, and simpler solutions could suffice.\n",
      "\n",
      "### Resource Utilization\n",
      "- **Assessment**: Kubernetes is efficient in managing resources, but this efficiency is more beneficial for larger, more complex projects.\n",
      "- **Conclusion**: For a small-scale project, the resource management capabilities of Kubernetes may not provide significant advantages.\n",
      "\n",
      "### Vendor Lock-in and Flexibility\n",
      "- **Assessment**: Kubernetes offers flexibility and reduces vendor lock-in.\n",
      "- **Conclusion**: While this is a benefit, it is less relevant for a small-scale, internal tool with minimal complexity.\n",
      "\n",
      "### Recommendation\n",
      "Given the factors above, Kubernetes does not appear to be an appropriate choice for this project. The complexity, overhead, and learning curve associated with Kubernetes are not justified for a small-scale, monolithic application with minimal scalability requirements and a team lacking containerization experience.\n",
      "\n",
      "### Alternative Recommendations\n",
      "- **Docker Compose**: For containerization without the complexity of Kubernetes, Docker Compose could be a simpler alternative.\n",
      "- **Traditional Deployment**: Given the simplicity of the project, traditional deployment methods (e.g., deploying directly on VMs or physical servers) might be the most straightforward and cost-effective approach.\n",
      "- **Platform-as-a-Service (PaaS)**: Consider using a PaaS solution like Heroku or Google App Engine, which can simplify deployment and management without the need for deep containerization expertise.\n",
      "\n",
      "In summary, Kubernetes is not recommended for this project. Simpler deployment solutions would be more appropriate and cost-effective.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "badFit=\"\"\"\n",
    "This project is a small-scale internal tool for automating office inventory management, expected to have a stable and predictable load with minimal scalability requirements. \n",
    "The application is built with a simple, monolithic architecture and will be deployed on a few servers. \n",
    "The team lacks experience with containerization and Kubernetes, and the simplicity of the project does not justify the overhead and complexity of implementing a Kubernetes environment.\n",
    "\"\"\"\n",
    "\n",
    "result = await kernel.invoke(kubernetesPlugin[\"KubeMatch\"], input=badFit)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add native code as a plugin\n",
    "The plugin described below allows us to **execute kubectl** commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class ExecuteKubectlPlugin:\n",
    "    \"\"\"\n",
    "    Description: This plugin is used to execute kubectl commands\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Execute kubectl command\",\n",
    "        name=\"ExecuteKubectl\",\n",
    "    )\n",
    "    def execute_kubectl_command (self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Execute kubectl command\n",
    "        \"\"\"\n",
    "        try:\n",
    "            command = input.replace(\"kubectl\", \"\").strip() # remove kubectl from the input\n",
    "            result = subprocess.run(['kubectl'] + command.split(), capture_output=True, text=True, check=True)\n",
    "\n",
    "            print(result.stdout)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error executing kubectl command: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the native plugin to the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_kubectl_plugin = kernel.add_plugin(plugin=ExecuteKubectlPlugin(), plugin_name=\"ExecuteKubectlPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes control plane is running at https://kufstein-demo-dns-iedm2rw6.hcp.germanywestcentral.azmk8s.io:443\n",
      "CoreDNS is running at https://kufstein-demo-dns-iedm2rw6.hcp.germanywestcentral.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n",
      "Metrics-server is running at https://kufstein-demo-dns-iedm2rw6.hcp.germanywestcentral.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execute_kubectl_fnc = execute_kubectl_plugin[\"ExecuteKubectl\"]\n",
    "kubectl_output = await execute_kubectl_fnc(kernel,input=\"kubectl cluster-info\")\n",
    "print(kubectl_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Planner ðŸª„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace \"test-kuh\" deleted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.planners.function_calling_stepwise_planner import (\n",
    "    FunctionCallingStepwisePlanner,\n",
    "    FunctionCallingStepwisePlannerOptions,\n",
    ")\n",
    "\n",
    "question = \"lÃ¶sche den Namespace test-kuh\"\n",
    "\n",
    "options = FunctionCallingStepwisePlannerOptions(\n",
    "    max_iterations=5,\n",
    "    max_tokens=4000,\n",
    ")\n",
    "\n",
    "planner = FunctionCallingStepwisePlanner(service_id=service_id, options=options)\n",
    "result = await planner.invoke(kernel, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history: <chat_history><message role=\"user\"><text>Original request: Zeige mir alle pods im system ns\n",
      "\n",
      "You are in the process of helping the user fulfill this request using the following plan:\n",
      "Um alle Pods im Namespace \"system\" anzuzeigen, werde ich die folgenden Schritte ausfÃ¼hren:\n",
      "\n",
      "1. Verwende die Funktion `KubernetesPlugin-KubectlPrompt`, um den natÃ¼rlichen Sprachbefehl in einen `kubectl`-Befehl zu konvertieren.\n",
      "2. Verwende die Funktion `ExecuteKubectlPlugin-ExecuteKubectl`, um den generierten `kubectl`-Befehl auszufÃ¼hren.\n",
      "\n",
      "Lass uns beginnen.\n",
      "\n",
      "**Schritt 1:** Konvertiere den natÃ¼rlichen Sprachbefehl in einen `kubectl`-Befehl.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"input\": {\n",
      "    \"input\": \"Zeige mir alle Pods im Namespace system\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "The user will ask you for help with each step.</text></message><message role=\"user\"><text>Perform the next step of the plan if there is more work to do. When you have reached a final answer, use the UserInteraction-SendFinalAnswer function to communicate this back to the user.</text></message><message ai_model_id=\"gpt-4o\" role=\"assistant\" finish_reason=\"tool_calls\"><function_call id=\"call_zsOWTv0hkJvbZndxqgMdNMav\" name=\"KubernetesPlugin-KubectlPrompt\">{}</function_call></message><message role=\"tool\"><function_result id=\"call_zsOWTv0hkJvbZndxqgMdNMav\" name=\"KubernetesPlugin-KubectlPrompt\">There are `1` tool call arguments required and only `0` received. The required arguments are: ['input']. Please provide the required arguments and try again.</function_result></message><message role=\"user\"><text>Perform the next step of the plan if there is more work to do. When you have reached a final answer, use the UserInteraction-SendFinalAnswer function to communicate this back to the user.</text></message><message ai_model_id=\"gpt-4o\" role=\"assistant\" finish_reason=\"tool_calls\"><function_call id=\"call_vq3cZO2UBRhDljKftQfUxGzg\" name=\"KubernetesPlugin-KubectlPrompt\">{\n",
      "  \"input\": \"Zeige mir alle Pods im Namespace system\"\n",
      "}</function_call></message><message role=\"tool\"><function_result id=\"call_vq3cZO2UBRhDljKftQfUxGzg\" name=\"KubernetesPlugin-KubectlPrompt\">[ChatMessageContent(inner_content=ChatCompletion(id='chatcmpl-AFjhjBHyIdDtUPQfcWmq1EYhF8Tdh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```sh\\nkubectl get pods -n system\\n```', refusal=None, role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1728313971, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_67802d9a6d', usage=CompletionUsage(completion_tokens=12, prompt_tokens=61, total_tokens=73, completion_tokens_details=None, prompt_tokens_details=None), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}]), ai_model_id='gpt-4o', metadata={'logprobs': None, 'id': 'chatcmpl-AFjhjBHyIdDtUPQfcWmq1EYhF8Tdh', 'created': 1728313971, 'system_fingerprint': 'fp_67802d9a6d', 'usage': CompletionUsage(prompt_tokens=61, completion_tokens=12)}, content_type='message', role=&lt;AuthorRole.ASSISTANT: 'assistant'&gt;, name=None, items=[TextContent(inner_content=None, ai_model_id=None, metadata={}, content_type='text', text='```sh\\nkubectl get pods -n system\\n```', encoding=None)], encoding=None, finish_reason=&lt;FinishReason.STOP: 'stop'&gt;)]</function_result></message><message role=\"user\"><text>Perform the next step of the plan if there is more work to do. When you have reached a final answer, use the UserInteraction-SendFinalAnswer function to communicate this back to the user.</text></message><message ai_model_id=\"gpt-4o\" role=\"assistant\" finish_reason=\"tool_calls\"><function_call id=\"call_s96Kvw9vN7NsEoXeOuSFaP8J\" name=\"ExecuteKubectlPlugin-ExecuteKubectl\">{\"input\":\"kubectl get pods -n system\"}</function_call></message><message role=\"tool\"><function_result id=\"call_s96Kvw9vN7NsEoXeOuSFaP8J\" name=\"ExecuteKubectlPlugin-ExecuteKubectl\">None</function_result></message><message role=\"user\"><text>Perform the next step of the plan if there is more work to do. When you have reached a final answer, use the UserInteraction-SendFinalAnswer function to communicate this back to the user.</text></message><message ai_model_id=\"gpt-4o\" role=\"assistant\" finish_reason=\"tool_calls\"><function_call id=\"call_BoZza17l1AAIuhk2cqZU7JBS\" name=\"UserInteraction-SendFinalAnswer\">{\"answer\":\"Ich konnte keine Antwort von dem Kubernetes-Cluster erhalten. Bitte Ã¼berprÃ¼fen Sie Ihre Kubernetes-Konfiguration und stellen Sie sicher, dass der Cluster erreichbar ist.\"}</function_call></message></chat_history>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Chat history: {result.chat_history}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
